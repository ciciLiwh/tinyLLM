{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "a7c90d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "\n",
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self, dim, eps=1e-6):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(dim))\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B, T, C)\n",
    "        norm = x.pow(2).mean(dim=-1, keepdim=True)\n",
    "        x = x * torch.rsqrt(norm + self.eps)\n",
    "        return self.weight * x\n",
    "    \n",
    "class RotaryEmbedding(nn.Module):\n",
    "    def __init__(self, dim, max_seq_len=2048):\n",
    "        super().__init__()\n",
    "        inv_freq = 1.0 / (10000 ** (torch.arange(0, dim, 2).float() / dim))\n",
    "        t = torch.arange(max_seq_len).float()\n",
    "        freqs = torch.einsum(\"i,j->ij\", t, inv_freq)\n",
    "        self.register_buffer(\"cos\", freqs.cos())\n",
    "        self.register_buffer(\"sin\", freqs.sin())\n",
    "\n",
    "    def apply_rope(self, x, seq_len):\n",
    "        cos = self.cos[:seq_len][None, :, None, :]\n",
    "        sin = self.sin[:seq_len][None, :, None, :]\n",
    "\n",
    "        x1, x2 = x[..., ::2], x[..., 1::2]\n",
    "        return torch.cat([x1 * cos - x2 * sin,\n",
    "                          x1 * sin + x2 * cos], dim=-1)\n",
    "\n",
    "class GQASelfAttention(nn.Module):\n",
    "    def __init__(self, dim, num_q_heads, num_kv_heads):\n",
    "        super().__init__()\n",
    "        assert num_q_heads % num_kv_heads == 0\n",
    "        self.num_q_heads = num_q_heads\n",
    "        self.num_kv_heads = num_kv_heads\n",
    "        self.head_dim = dim // num_q_heads\n",
    "        self.qNorm = RMSNorm(self.head_dim)\n",
    "        self.kNorm = RMSNorm(self.head_dim)\n",
    "\n",
    "        self.q_proj = nn.Linear(dim, dim)\n",
    "        self.k_proj = nn.Linear(dim, self.head_dim * num_kv_heads)\n",
    "        self.v_proj = nn.Linear(dim, self.head_dim * num_kv_heads)\n",
    "        self.gate_proj = nn.Linear(self.head_dim, self.head_dim, bias=True)\n",
    "        self.out_proj = nn.Linear(dim, dim)\n",
    "\n",
    "    def forward(self, x, rope: RotaryEmbedding):\n",
    "        B, T, C = x.shape\n",
    "\n",
    "        q = self.q_proj(x).view(B, T, self.num_q_heads, self.head_dim)\n",
    "        k = self.k_proj(x).view(B, T, self.num_kv_heads, self.head_dim)\n",
    "        v = self.v_proj(x).view(B, T, self.num_kv_heads, self.head_dim)\n",
    "\n",
    "        q = rope.apply_rope(q, T)\n",
    "        k = rope.apply_rope(k, T)\n",
    "\n",
    "        q = self.qNorm(q)\n",
    "        k = self.kNorm(k)\n",
    "\n",
    "        # GQA: repeat kv heads\n",
    "        repeat = self.num_q_heads // self.num_kv_heads\n",
    "        k = k.repeat_interleave(repeat, dim=2)\n",
    "        v = v.repeat_interleave(repeat, dim=2)\n",
    "\n",
    "        attn = torch.einsum(\"bthd,bshd->bhts\", q, k) / math.sqrt(self.head_dim)\n",
    "\n",
    "        causal_mask = torch.tril(torch.ones(T, T, device=x.device))\n",
    "        attn = attn.masked_fill(causal_mask == 0, float(\"-inf\"))\n",
    "\n",
    "        attn = F.softmax(attn, dim=-1)\n",
    "        out = torch.einsum(\"bhts,bshd->bthd\", attn, v)\n",
    "        gate = torch.sigmoid(self.gate_proj(out))\n",
    "        out = out * gate\n",
    "        out = out.reshape(B, T, C)\n",
    "        return self.out_proj(out)\n",
    "    \n",
    "class MoE(nn.Module):\n",
    "    def __init__(self, dim, hidden_dim, num_experts):\n",
    "        super().__init__()\n",
    "        self.router = nn.Linear(dim, num_experts)\n",
    "        self.experts = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Linear(dim, hidden_dim),\n",
    "                nn.GELU(),\n",
    "                nn.Linear(hidden_dim, dim)\n",
    "            ) for _ in range(num_experts)\n",
    "        ])\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        logits = self.router(x)\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "\n",
    "        top1 = probs.argmax(dim=-1)\n",
    "        out = torch.zeros_like(x)\n",
    "\n",
    "        for i, expert in enumerate(self.experts):\n",
    "            mask = top1 == i\n",
    "            if mask.any():\n",
    "                out[mask] = expert(x[mask])\n",
    "\n",
    "        return out\n",
    "    \n",
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, dim, num_q_heads, num_kv_heads, moe_hidden, num_experts):\n",
    "        super().__init__()\n",
    "        self.ln1 = RMSNorm(dim)\n",
    "        self.attn = GQASelfAttention(dim, num_q_heads, num_kv_heads)\n",
    "\n",
    "        self.ln2 = RMSNorm(dim)\n",
    "        self.moe = MoE(dim, moe_hidden, num_experts)\n",
    "\n",
    "    def forward(self, x, rope):\n",
    "        x = x + self.attn(self.ln1(x), rope)\n",
    "        x = x + self.moe(self.ln2(x))\n",
    "        return x\n",
    "    \n",
    "class DecoderOnlyTransformer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size,\n",
    "        dim=512,\n",
    "        num_layers=6,\n",
    "        num_q_heads=8,\n",
    "        num_kv_heads=2,\n",
    "        moe_hidden=2048,\n",
    "        num_experts=4,\n",
    "        max_seq_len=2048\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embed = nn.Embedding(vocab_size, dim)\n",
    "        self.rope = RotaryEmbedding(dim // num_q_heads, max_seq_len)\n",
    "\n",
    "        self.layers = nn.ModuleList([\n",
    "            DecoderBlock(dim, num_q_heads, num_kv_heads, moe_hidden, num_experts)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "        self.norm = RMSNorm(dim)\n",
    "        self.lm_head = nn.Linear(dim, vocab_size, bias=False)\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        x = self.embed(input_ids)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, self.rope)\n",
    "        x = self.norm(x)\n",
    "        return self.lm_head(x)\n",
    "    \n",
    "def main():\n",
    "    # ====== device (MPS 优先) ======\n",
    "    if torch.backends.mps.is_available():\n",
    "        device = torch.device(\"mps\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "    print(\"Using device:\", device)\n",
    "\n",
    "    # ====== 超小模型，防止 M 芯片炸显存 ======\n",
    "    vocab_size = 100\n",
    "    model = DecoderOnlyTransformer(\n",
    "        vocab_size=vocab_size,\n",
    "        dim=128,\n",
    "        num_layers=2,\n",
    "        num_q_heads=4,\n",
    "        num_kv_heads=2,\n",
    "        moe_hidden=256,\n",
    "        num_experts=2,\n",
    "        max_seq_len=64\n",
    "    ).to(device)\n",
    "\n",
    "    # ====== dummy input ======\n",
    "    batch_size = 2\n",
    "    seq_len = 16\n",
    "    input_ids = torch.randint(\n",
    "        0, vocab_size, (batch_size, seq_len),\n",
    "        device=device\n",
    "    )\n",
    "\n",
    "    # ====== forward ======\n",
    "    logits = model(input_ids)\n",
    "    print(\"logits shape:\", logits.shape)  # (B, T, vocab)\n",
    "\n",
    "    # ====== language modeling loss ======\n",
    "    loss = F.cross_entropy(\n",
    "        logits[:, :-1].reshape(-1, vocab_size),\n",
    "        input_ids[:, 1:].reshape(-1)\n",
    "    )\n",
    "    print(\"loss:\", loss.item())\n",
    "\n",
    "    # ====== backward ======\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    print(\"Backward & step OK ✅\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "ce8c3cfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n",
      "logits shape: torch.Size([2, 16, 100])\n",
      "loss: 4.637609958648682\n",
      "Backward & step OK ✅\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
