{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4b13e99b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# 假设JSON数据已经保存在变量中，如果是从文件读取，可以使用以下代码：\n",
    "with open('宋词_pretrain.json', 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "def extract_text_from_json(data):\n",
    "    result = []\n",
    "    for poem in data:\n",
    "        # 添加词牌名\n",
    "        result.append(poem[\"rhythmic\"])\n",
    "        \n",
    "        # 添加所有句子\n",
    "        for sentence in poem[\"paragraphs\"]:\n",
    "            result.append(sentence)\n",
    "        \n",
    "        # 添加分隔符\n",
    "        result.append(\"<|endoftext|>\")\n",
    "    \n",
    "    return \"\\n\".join(result)\n",
    "\n",
    "# 提取文本\n",
    "extracted_text = extract_text_from_json(data)\n",
    "# print(extracted_text)\n",
    "\n",
    "# 如果需要保存到文件\n",
    "with open('pretrain.txt', 'w', encoding='utf-8') as f:\n",
    "    f.write(extracted_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1e56d13e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('pretrain.txt', 'r', encoding='utf-8') as f:\n",
    "    content = f.read()\n",
    "\n",
    "# 移除所有 <|endoftext|>\n",
    "content = content.replace('<|endoftext|>', '')\n",
    "\n",
    "with open('pretrain.txt', 'w', encoding='utf-8') as f:\n",
    "    f.write(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "267244b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在转换数据...\n",
      "转换完成！共处理 280 条数据\n",
      "输出文件: 宋词_SFT.jsonl\n",
      "\n",
      "预览生成的SFT数据：\n",
      "==================================================\n",
      "样本 1:\n",
      "Instruction: 湘春夜月\n",
      "Input: \n",
      "Output: 近清明。\n",
      "翠禽枝上消魂。\n",
      "可惜一片清歌，都付与黄昏。\n",
      "欲共柳花低诉，怕柳花轻薄，不解伤春。\n",
      "念楚乡旅宿，柔情别绪，谁与温存。\n",
      "空樽夜泣，青山不语，残月当门。\n",
      "翠玉楼前，惟是有、一波湘水，摇荡湘云。\n",
      "天...\n",
      "--------------------------------------------------\n",
      "样本 2:\n",
      "Instruction: 瑞鹤仙\n",
      "Input: \n",
      "Output: 湿云黏雁影。\n",
      "望征路愁迷，离绪难整。\n",
      "千金买光景。\n",
      "但疏钟催晓，乱鸦啼暝。\n",
      "花暗省。\n",
      "许多情、相逢梦境。\n",
      "便行云、都不归来，也合寄将音信。\n",
      "孤迥。\n",
      "盟鸾心在，跨鹤程高，後期无准。\n",
      "情丝待翦。\n",
      "翻惹得，...\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "def convert_ci_data_to_sft_format(input_file, output_file):\n",
    "    \"\"\"\n",
    "    将宋词JSON数据转换为SFT训练格式\n",
    "    \n",
    "    Args:\n",
    "        input_file: 输入的JSON文件路径\n",
    "        output_file: 输出的JSONL文件路径\n",
    "    \"\"\"\n",
    "    # 读取原始数据\n",
    "    with open(input_file, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    # 准备输出数据\n",
    "    sft_data = []\n",
    "    \n",
    "    for item in data:\n",
    "        # 构建instruction：词牌名\n",
    "        instruction = f\"{item['rhythmic']}\"\n",
    "        \n",
    "        # 构建output：将所有段落用换行符连接，并添加<eos>标记\n",
    "        paragraphs = item['paragraphs']\n",
    "        output_text = \"\\n\".join(paragraphs) + \"\\n\"\n",
    "        \n",
    "        # 创建SFT格式的记录\n",
    "        sft_record = {\n",
    "            \"instruction\": instruction,\n",
    "            \"input\": \"\",  # 输入为空\n",
    "            \"output\": output_text\n",
    "        }\n",
    "        \n",
    "        sft_data.append(sft_record)\n",
    "    \n",
    "    # 保存为JSONL格式（每行一个JSON对象）\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        for record in sft_data:\n",
    "            f.write(json.dumps(record, ensure_ascii=False) + '\\n')\n",
    "    \n",
    "    print(f\"转换完成！共处理 {len(sft_data)} 条数据\")\n",
    "    print(f\"输出文件: {output_file}\")     \n",
    "\n",
    "\n",
    "def preview_sft_data(output_file, num_samples=2):\n",
    "    \"\"\"预览生成的SFT数据\"\"\"\n",
    "    print(\"\\n预览生成的SFT数据：\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    with open(output_file, 'r', encoding='utf-8') as f:\n",
    "        for i, line in enumerate(f):\n",
    "            if i >= num_samples:\n",
    "                break\n",
    "            record = json.loads(line.strip())\n",
    "            print(f\"样本 {i+1}:\")\n",
    "            print(f\"Instruction: {record['instruction']}\")\n",
    "            print(f\"Input: {record['input']}\")\n",
    "            print(f\"Output: {record['output'][:100]}...\")  # 只显示前100个字符\n",
    "            print(\"-\" * 50)\n",
    "\n",
    "def main():\n",
    "    # 输入文件路径（假设您的数据保存在这个文件）\n",
    "    input_file = \"宋词_SFT.json\"  # 修改为您的实际文件路径\n",
    "    \n",
    "    # 输出文件路径（符合原代码要求的路径）\n",
    "    output_file = \"宋词_SFT.jsonl\"\n",
    "    \n",
    "    # 检查输入文件是否存在\n",
    "    if not Path(input_file).exists():\n",
    "        print(f\"错误：输入文件 {input_file} 不存在！\")\n",
    "        print(\"请将您的JSON数据保存为 ci_data.json 或修改 input_file 变量\")\n",
    "        return\n",
    "    \n",
    "    # 使用版本1进行转换（最接近您的要求）\n",
    "    print(\"正在转换数据...\")\n",
    "    convert_ci_data_to_sft_format(input_file, output_file)\n",
    "    \n",
    "    # 预览生成的数据\n",
    "    preview_sft_data(output_file)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2f76b94f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['bbpe/vocab.json', 'bbpe/merges.txt']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from tokenizers import ByteLevelBPETokenizer\n",
    "\n",
    "# 输入文件路径\n",
    "input_file_path = './pretrain.txt'\n",
    "\n",
    "# 读取原始文本数据\n",
    "with open(input_file_path, 'r', encoding='utf-8') as f:\n",
    "    data = f.read()\n",
    "\n",
    "# 初始化Byte-level BPE分词器\n",
    "tokenizer = ByteLevelBPETokenizer()\n",
    "\n",
    "# 训练分词器\n",
    "tokenizer.train(\n",
    "    files=[input_file_path],      # 训练文件\n",
    "    vocab_size=2048,              # 词汇表大小\n",
    "    min_frequency=2,              # 最小出现频率\n",
    "    special_tokens=[\"<|endoftext|>\"],  # 特殊标记\n",
    ")\n",
    "\n",
    "# 创建保存分词器模型的目录\n",
    "if not os.path.exists(\"bbpe\"):\n",
    "    os.mkdir(\"bbpe\")\n",
    "    \n",
    "# 保存训练好的分词器模型（词汇表和合并规则）\n",
    "tokenizer.save_model(\"bbpe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bb2d6d5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "总token数: 430575\n",
      "eostoken数量: 4998\n",
      "诗词数量: 4998\n",
      "\n",
      "前50个token ID:\n",
      "[ 926  199  406  315  227  868  839  584  402  326  258  199  528  898\n",
      "  625 1142 1503  824  258  199  637  971  396  243  541  360  258  199\n",
      "  507  506 1997  409  258  199 1224 1797  343   97  258  199  603  496\n",
      " 1957 1247  258  199  700  497  682  961]\n"
     ]
    }
   ],
   "source": [
    "# 重新加载分词器\n",
    "tokenizer = ByteLevelBPETokenizer(\n",
    "    \"bbpe/vocab.json\",    # 词汇表文件\n",
    "    \"bbpe/merges.txt\",    # BPE合并规则文件\n",
    ")\n",
    "\n",
    "# 读取文本数据\n",
    "with open('pretrain.txt', 'r', encoding='utf-8') as f:\n",
    "    data = f.read()\n",
    "\n",
    "# 按空行分割成一首首词\n",
    "poems = data.strip().split('\\n\\n')\n",
    "\n",
    "all_token_ids = []\n",
    "\n",
    "for i, poem in enumerate(poems):\n",
    "    # 对每首词进行编码\n",
    "    poem_ids = tokenizer.encode(poem).ids\n",
    "    \n",
    "    # 添加到总列表\n",
    "    all_token_ids.extend(poem_ids)\n",
    "    all_token_ids.append(0)\n",
    "\n",
    "# 将ID列表转换为numpy数组，使用uint16类型以节省空间\n",
    "pretrain_ids = np.array(all_token_ids, dtype=np.uint16)\n",
    "\n",
    "# 将token ID序列保存为二进制文件\n",
    "pretrain_ids.tofile('./pretrain.bin')\n",
    "\n",
    "# 验证一下\n",
    "print(f\"总token数: {len(pretrain_ids)}\")\n",
    "print(f\"eostoken数量: {np.sum(pretrain_ids == 0)}\")\n",
    "print(f\"诗词数量: {len(poems)}\")\n",
    "\n",
    "# 查看前50个token（包含eostoken）\n",
    "print(\"\\n前50个token ID:\")\n",
    "print(pretrain_ids[:50])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
